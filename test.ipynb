{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.tensor([[[1, 2],\n",
    "                   [3, 4]],\n",
    "                  [[5, 6],\n",
    "                   [7, 8]]])\n",
    "print(t.size())\n",
    "print(t.view(t.size(0),-1).shape)\n",
    "t.view(t.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 2, 3, 6, 1, 5]"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "src = [0,2,3,6,1,5]\n",
    "src = np.array(src)\n",
    "src.argsort().argsort()+1\n",
    "src.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = a[b]\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import  SummaryWriter\n",
    "logger = SummaryWriter(\"log/test\")\n",
    "logger.add_text(\"Best Acc\", str(12), global_step=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "26.37000274658203"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "test_acc = pd.read_csv('./runs/cifar100_110_case2_1/test_acc.csv')\n",
    "best_error = 100 - test_acc['Value'].max()\n",
    "best_error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'torch.save(model.state_dict(), \"test.pth\")\\nx = torch.randn(128, 3, 36, 36).cuda()  # 随机生成一个输入\\ntorch.onnx.export(model,x , \\'test.pth\\')  # 将 pytorch 模型以 onnx 格式导出并保存\\nnetron.start(\\'test.pth\\')  # 输出网络结构\\n\\nmodelData = \"./demo.pth\"  # 定义模型数据保存的路径\\nnetron.start(\\'test.pth\\')  # 输出网络结构'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from resnet_2 import resnet110\n",
    "import netron\n",
    "import torch\n",
    "from torch.utils import tensorboard\n",
    "model = resnet110(case=4).cuda()\n",
    "ts = tensorboard.SummaryWriter(log_dir=\"./log/case4\")\n",
    "ts.add_graph(model ,torch.randn(128, 3, 32, 32).cuda())\n",
    "'''torch.save(model.state_dict(), \"test.pth\")\n",
    "x = torch.randn(128, 3, 36, 36).cuda()  # 随机生成一个输入\n",
    "torch.onnx.export(model,x , 'test.pth')  # 将 pytorch 模型以 onnx 格式导出并保存\n",
    "netron.start('test.pth')  # 输出网络结构\n",
    "\n",
    "modelData = \"./demo.pth\"  # 定义模型数据保存的路径\n",
    "netron.start('test.pth')  # 输出网络结构'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from resnet_2 import resnet164\n",
    "import torch\n",
    "from torch.utils import tensorboard\n",
    "model = resnet164(case=0).cuda()\n",
    "ts = tensorboard.SummaryWriter(log_dir=\"./log/resnet164/model0\")\n",
    "\n",
    "ts.add_graph(model ,torch.randn(64, 3, 32, 32).cuda())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Case only support {0,1,2,3,4}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m case\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m case \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m}:\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCase only support \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m0,1,2,3,4}\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Case only support {0,1,2,3,4}"
     ]
    }
   ],
   "source": [
    "case=1\n",
    "if case not in {0, 1, 2, 3, 4}:\n",
    "    raise ValueError('Case only support {0,1,2,3,4}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.09069127, -0.03566648,  0.0599517 ],\n       [-0.03566648,  0.06645802, -0.00721617],\n       [ 0.0599517 , -0.00721617,  0.08830344]])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "i = np.identity(5)\n",
    "i1 = np.array([[1,1,1,1,1]]).T\n",
    "i_ = 1/5 * (i - 1/5 * (i1@i1.T))\n",
    "x = np.random.random([3,5])\n",
    "dp = np.random.random([3,3])\n",
    "p = x@i_@x.T\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1., 2.],\n         [3., 4.]], requires_grad=True),\n tensor([[0.6892, 0.2267],\n         [0.0124, 0.5823]]),\n tensor([[4., 3.],\n         [2., 1.]], requires_grad=True))"
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Function\n",
    "class MultiplyAdd(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b):\n",
    "        ctx.save_for_backward(x, )#存储用来反向传播的参数\n",
    "        output = w*x +b\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors #deprecated,现在使用saved_tensors\n",
    "        grad_w = grad_output * x\n",
    "        grad_b = grad_output * 1\n",
    "        return grad_w, None, grad_b\n",
    "\n",
    "w = torch.tensor([[1.,2],[3,4]], requires_grad=True)\n",
    "x = torch.rand(2, 2)\n",
    "b = torch.tensor([[4.,3],[2,1]], requires_grad=True)\n",
    "w,x,b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.6892, 0.2267],\n         [0.0124, 0.5823]]),\n tensor([[1., 1.],\n         [1., 1.]]))"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_func = MultiplyAdd()\n",
    "out = ag_func.apply(w, x, b)\n",
    "out.backward(torch.ones(2,2), retain_graph=True)\n",
    "w.grad, b.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.MultiplyAddBackward object at 0x0000014F2AFB52E0>\n",
      "((<AccumulateGrad object at 0x0000014F2AF82040>, 0), (None, 0), (<AccumulateGrad object at 0x0000014F2A53F250>, 0))\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "tensor([[4., 3.],\n",
      "        [2., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(out.grad_fn)\n",
    "print(out.grad_fn.next_functions)\n",
    "# print(out.grad_fn.saved_tensors)\n",
    "print(out.grad_fn.next_functions[0][0].variable)\n",
    "print(out.grad_fn.next_functions[2][0].variable)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "'graph.pdf'"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "seen = set()\n",
    "\n",
    "def size_to_str(size):\n",
    "    return '(' + (', ').join(['%d' % v for v in size]) + ')'\n",
    "\n",
    "def add_nodes(var):\n",
    "\n",
    "    if var not in seen:\n",
    "        if torch.is_tensor(var):\n",
    "            # note: this used to show .saved_tensors in pytorch0.2, but stopped\n",
    "            # working as it was moved to ATen and Variable-Tensor merged\n",
    "            dot.node(str(id(var)), size_to_str(var.size()), fillcolor='yellow')\n",
    "        elif hasattr(var, 'variable'):\n",
    "            u = var.variable\n",
    "            node_name = size_to_str(u.size())\n",
    "            dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "        else:\n",
    "            dot.node(str(id(var)), str(type(var).__name__))\n",
    "\n",
    "            seen.add(var)\n",
    "        if hasattr(var, 'next_functions'):\n",
    "            for u in var.next_functions:\n",
    "                if u[0] is not None:\n",
    "                    dot.edge(str(id(u[0])), str(id(var)))\n",
    "                    add_nodes(u[0])\n",
    "        if hasattr(var, 'saved_tensors'):\n",
    "            for t in var.saved_tensors:\n",
    "                dot.edge(str(id(t)), str(id(var)))\n",
    "                add_nodes(t)\n",
    "dot.node('Output', 'out\\n'+size_to_str(out.size()))\n",
    "dot.edge( str(id(out.grad_fn)),'Output')\n",
    "add_nodes(out.grad_fn)\n",
    "dot.render(('graph'), view=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             432\n",
      "            Conv2d-2           [-1, 64, 32, 32]           1,024\n",
      "       BatchNorm2d-3           [-1, 64, 32, 32]             128\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "              ReLU-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "           Conv2d-10           [-1, 64, 32, 32]           1,024\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "             ReLU-12           [-1, 64, 32, 32]               0\n",
      "       Bottleneck-13           [-1, 64, 32, 32]               0\n",
      "           Conv2d-14           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "             ReLU-16           [-1, 64, 32, 32]               0\n",
      "           Conv2d-17           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-18           [-1, 64, 32, 32]             128\n",
      "             ReLU-19           [-1, 64, 32, 32]               0\n",
      "           Conv2d-20           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-21           [-1, 64, 32, 32]             128\n",
      "             ReLU-22           [-1, 64, 32, 32]               0\n",
      "       Bottleneck-23           [-1, 64, 32, 32]               0\n",
      "           Conv2d-24           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-25           [-1, 64, 32, 32]             128\n",
      "             ReLU-26           [-1, 64, 32, 32]               0\n",
      "           Conv2d-27           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-28           [-1, 64, 32, 32]             128\n",
      "             ReLU-29           [-1, 64, 32, 32]               0\n",
      "           Conv2d-30           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-31           [-1, 64, 32, 32]             128\n",
      "             ReLU-32           [-1, 64, 32, 32]               0\n",
      "       Bottleneck-33           [-1, 64, 32, 32]               0\n",
      "           Conv2d-34           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-35           [-1, 64, 32, 32]             128\n",
      "             ReLU-36           [-1, 64, 32, 32]               0\n",
      "           Conv2d-37           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-38           [-1, 64, 32, 32]             128\n",
      "             ReLU-39           [-1, 64, 32, 32]               0\n",
      "           Conv2d-40           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-41           [-1, 64, 32, 32]             128\n",
      "             ReLU-42           [-1, 64, 32, 32]               0\n",
      "       Bottleneck-43           [-1, 64, 32, 32]               0\n",
      "           Conv2d-44           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-45           [-1, 64, 32, 32]             128\n",
      "             ReLU-46           [-1, 64, 32, 32]               0\n",
      "           Conv2d-47           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-48           [-1, 64, 32, 32]             128\n",
      "             ReLU-49           [-1, 64, 32, 32]               0\n",
      "           Conv2d-50           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-51           [-1, 64, 32, 32]             128\n",
      "             ReLU-52           [-1, 64, 32, 32]               0\n",
      "       Bottleneck-53           [-1, 64, 32, 32]               0\n",
      "           Conv2d-54           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-55           [-1, 64, 32, 32]             128\n",
      "             ReLU-56           [-1, 64, 32, 32]               0\n",
      "           Conv2d-57           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-58           [-1, 64, 32, 32]             128\n",
      "             ReLU-59           [-1, 64, 32, 32]               0\n",
      "           Conv2d-60           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-61           [-1, 64, 32, 32]             128\n",
      "             ReLU-62           [-1, 64, 32, 32]               0\n",
      "       Bottleneck-63           [-1, 64, 32, 32]               0\n",
      "           Conv2d-64           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-65           [-1, 64, 32, 32]             128\n",
      "             ReLU-66           [-1, 64, 32, 32]               0\n",
      "           Conv2d-67           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-68           [-1, 64, 32, 32]             128\n",
      "             ReLU-69           [-1, 64, 32, 32]               0\n",
      "           Conv2d-70           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-71           [-1, 64, 32, 32]             128\n",
      "             ReLU-72           [-1, 64, 32, 32]               0\n",
      "       Bottleneck-73           [-1, 64, 32, 32]               0\n",
      "           Conv2d-74           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-75           [-1, 64, 32, 32]             128\n",
      "             ReLU-76           [-1, 64, 32, 32]               0\n",
      "           Conv2d-77           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-78           [-1, 64, 32, 32]             128\n",
      "             ReLU-79           [-1, 64, 32, 32]               0\n",
      "           Conv2d-80           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-81           [-1, 64, 32, 32]             128\n",
      "             ReLU-82           [-1, 64, 32, 32]               0\n",
      "       Bottleneck-83           [-1, 64, 32, 32]               0\n",
      "           Conv2d-84           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-85           [-1, 64, 32, 32]             128\n",
      "             ReLU-86           [-1, 64, 32, 32]               0\n",
      "           Conv2d-87           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-88           [-1, 64, 32, 32]             128\n",
      "             ReLU-89           [-1, 64, 32, 32]               0\n",
      "           Conv2d-90           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-91           [-1, 64, 32, 32]             128\n",
      "             ReLU-92           [-1, 64, 32, 32]               0\n",
      "       Bottleneck-93           [-1, 64, 32, 32]               0\n",
      "           Conv2d-94           [-1, 64, 32, 32]           4,096\n",
      "      BatchNorm2d-95           [-1, 64, 32, 32]             128\n",
      "             ReLU-96           [-1, 64, 32, 32]               0\n",
      "           Conv2d-97           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-98           [-1, 64, 32, 32]             128\n",
      "             ReLU-99           [-1, 64, 32, 32]               0\n",
      "          Conv2d-100           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-101           [-1, 64, 32, 32]             128\n",
      "            ReLU-102           [-1, 64, 32, 32]               0\n",
      "      Bottleneck-103           [-1, 64, 32, 32]               0\n",
      "          Conv2d-104           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-105           [-1, 64, 32, 32]             128\n",
      "            ReLU-106           [-1, 64, 32, 32]               0\n",
      "          Conv2d-107           [-1, 64, 32, 32]          36,864\n",
      "     BatchNorm2d-108           [-1, 64, 32, 32]             128\n",
      "            ReLU-109           [-1, 64, 32, 32]               0\n",
      "          Conv2d-110           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-111           [-1, 64, 32, 32]             128\n",
      "            ReLU-112           [-1, 64, 32, 32]               0\n",
      "      Bottleneck-113           [-1, 64, 32, 32]               0\n",
      "          Conv2d-114           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-115           [-1, 64, 32, 32]             128\n",
      "            ReLU-116           [-1, 64, 32, 32]               0\n",
      "          Conv2d-117           [-1, 64, 32, 32]          36,864\n",
      "     BatchNorm2d-118           [-1, 64, 32, 32]             128\n",
      "            ReLU-119           [-1, 64, 32, 32]               0\n",
      "          Conv2d-120           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-121           [-1, 64, 32, 32]             128\n",
      "            ReLU-122           [-1, 64, 32, 32]               0\n",
      "      Bottleneck-123           [-1, 64, 32, 32]               0\n",
      "          Conv2d-124           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-125           [-1, 64, 32, 32]             128\n",
      "            ReLU-126           [-1, 64, 32, 32]               0\n",
      "          Conv2d-127           [-1, 64, 32, 32]          36,864\n",
      "     BatchNorm2d-128           [-1, 64, 32, 32]             128\n",
      "            ReLU-129           [-1, 64, 32, 32]               0\n",
      "          Conv2d-130           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-131           [-1, 64, 32, 32]             128\n",
      "            ReLU-132           [-1, 64, 32, 32]               0\n",
      "      Bottleneck-133           [-1, 64, 32, 32]               0\n",
      "          Conv2d-134           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-135           [-1, 64, 32, 32]             128\n",
      "            ReLU-136           [-1, 64, 32, 32]               0\n",
      "          Conv2d-137           [-1, 64, 32, 32]          36,864\n",
      "     BatchNorm2d-138           [-1, 64, 32, 32]             128\n",
      "            ReLU-139           [-1, 64, 32, 32]               0\n",
      "          Conv2d-140           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-141           [-1, 64, 32, 32]             128\n",
      "            ReLU-142           [-1, 64, 32, 32]               0\n",
      "      Bottleneck-143           [-1, 64, 32, 32]               0\n",
      "          Conv2d-144           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-145           [-1, 64, 32, 32]             128\n",
      "            ReLU-146           [-1, 64, 32, 32]               0\n",
      "          Conv2d-147           [-1, 64, 32, 32]          36,864\n",
      "     BatchNorm2d-148           [-1, 64, 32, 32]             128\n",
      "            ReLU-149           [-1, 64, 32, 32]               0\n",
      "          Conv2d-150           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-151           [-1, 64, 32, 32]             128\n",
      "            ReLU-152           [-1, 64, 32, 32]               0\n",
      "      Bottleneck-153           [-1, 64, 32, 32]               0\n",
      "          Conv2d-154           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-155           [-1, 64, 32, 32]             128\n",
      "            ReLU-156           [-1, 64, 32, 32]               0\n",
      "          Conv2d-157           [-1, 64, 32, 32]          36,864\n",
      "     BatchNorm2d-158           [-1, 64, 32, 32]             128\n",
      "            ReLU-159           [-1, 64, 32, 32]               0\n",
      "          Conv2d-160           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-161           [-1, 64, 32, 32]             128\n",
      "            ReLU-162           [-1, 64, 32, 32]               0\n",
      "      Bottleneck-163           [-1, 64, 32, 32]               0\n",
      "          Conv2d-164           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-165           [-1, 64, 32, 32]             128\n",
      "            ReLU-166           [-1, 64, 32, 32]               0\n",
      "          Conv2d-167           [-1, 64, 32, 32]          36,864\n",
      "     BatchNorm2d-168           [-1, 64, 32, 32]             128\n",
      "            ReLU-169           [-1, 64, 32, 32]               0\n",
      "          Conv2d-170           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-171           [-1, 64, 32, 32]             128\n",
      "            ReLU-172           [-1, 64, 32, 32]               0\n",
      "      Bottleneck-173           [-1, 64, 32, 32]               0\n",
      "          Conv2d-174           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-175           [-1, 64, 32, 32]             128\n",
      "            ReLU-176           [-1, 64, 32, 32]               0\n",
      "          Conv2d-177           [-1, 64, 32, 32]          36,864\n",
      "     BatchNorm2d-178           [-1, 64, 32, 32]             128\n",
      "            ReLU-179           [-1, 64, 32, 32]               0\n",
      "          Conv2d-180           [-1, 64, 32, 32]           4,096\n",
      "     BatchNorm2d-181           [-1, 64, 32, 32]             128\n",
      "            ReLU-182           [-1, 64, 32, 32]               0\n",
      "      Bottleneck-183           [-1, 64, 32, 32]               0\n",
      "          Conv2d-184          [-1, 128, 32, 32]           8,192\n",
      "     BatchNorm2d-185          [-1, 128, 32, 32]             256\n",
      "            ReLU-186          [-1, 128, 32, 32]               0\n",
      "          Conv2d-187          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-188          [-1, 128, 16, 16]             256\n",
      "            ReLU-189          [-1, 128, 16, 16]               0\n",
      "          Conv2d-190          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-191          [-1, 128, 16, 16]             256\n",
      "          Conv2d-192          [-1, 128, 16, 16]           8,192\n",
      "     BatchNorm2d-193          [-1, 128, 16, 16]             256\n",
      "            ReLU-194          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-195          [-1, 128, 16, 16]               0\n",
      "          Conv2d-196          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-197          [-1, 128, 16, 16]             256\n",
      "            ReLU-198          [-1, 128, 16, 16]               0\n",
      "          Conv2d-199          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-200          [-1, 128, 16, 16]             256\n",
      "            ReLU-201          [-1, 128, 16, 16]               0\n",
      "          Conv2d-202          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-203          [-1, 128, 16, 16]             256\n",
      "            ReLU-204          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-205          [-1, 128, 16, 16]               0\n",
      "          Conv2d-206          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-207          [-1, 128, 16, 16]             256\n",
      "            ReLU-208          [-1, 128, 16, 16]               0\n",
      "          Conv2d-209          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-210          [-1, 128, 16, 16]             256\n",
      "            ReLU-211          [-1, 128, 16, 16]               0\n",
      "          Conv2d-212          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-213          [-1, 128, 16, 16]             256\n",
      "            ReLU-214          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-215          [-1, 128, 16, 16]               0\n",
      "          Conv2d-216          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-217          [-1, 128, 16, 16]             256\n",
      "            ReLU-218          [-1, 128, 16, 16]               0\n",
      "          Conv2d-219          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-220          [-1, 128, 16, 16]             256\n",
      "            ReLU-221          [-1, 128, 16, 16]               0\n",
      "          Conv2d-222          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-223          [-1, 128, 16, 16]             256\n",
      "            ReLU-224          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-225          [-1, 128, 16, 16]               0\n",
      "          Conv2d-226          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-227          [-1, 128, 16, 16]             256\n",
      "            ReLU-228          [-1, 128, 16, 16]               0\n",
      "          Conv2d-229          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-230          [-1, 128, 16, 16]             256\n",
      "            ReLU-231          [-1, 128, 16, 16]               0\n",
      "          Conv2d-232          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-233          [-1, 128, 16, 16]             256\n",
      "            ReLU-234          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-235          [-1, 128, 16, 16]               0\n",
      "          Conv2d-236          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-237          [-1, 128, 16, 16]             256\n",
      "            ReLU-238          [-1, 128, 16, 16]               0\n",
      "          Conv2d-239          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-240          [-1, 128, 16, 16]             256\n",
      "            ReLU-241          [-1, 128, 16, 16]               0\n",
      "          Conv2d-242          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-243          [-1, 128, 16, 16]             256\n",
      "            ReLU-244          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-245          [-1, 128, 16, 16]               0\n",
      "          Conv2d-246          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-247          [-1, 128, 16, 16]             256\n",
      "            ReLU-248          [-1, 128, 16, 16]               0\n",
      "          Conv2d-249          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-250          [-1, 128, 16, 16]             256\n",
      "            ReLU-251          [-1, 128, 16, 16]               0\n",
      "          Conv2d-252          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-253          [-1, 128, 16, 16]             256\n",
      "            ReLU-254          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-255          [-1, 128, 16, 16]               0\n",
      "          Conv2d-256          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-257          [-1, 128, 16, 16]             256\n",
      "            ReLU-258          [-1, 128, 16, 16]               0\n",
      "          Conv2d-259          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-260          [-1, 128, 16, 16]             256\n",
      "            ReLU-261          [-1, 128, 16, 16]               0\n",
      "          Conv2d-262          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-263          [-1, 128, 16, 16]             256\n",
      "            ReLU-264          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-265          [-1, 128, 16, 16]               0\n",
      "          Conv2d-266          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-267          [-1, 128, 16, 16]             256\n",
      "            ReLU-268          [-1, 128, 16, 16]               0\n",
      "          Conv2d-269          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-270          [-1, 128, 16, 16]             256\n",
      "            ReLU-271          [-1, 128, 16, 16]               0\n",
      "          Conv2d-272          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-273          [-1, 128, 16, 16]             256\n",
      "            ReLU-274          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-275          [-1, 128, 16, 16]               0\n",
      "          Conv2d-276          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-277          [-1, 128, 16, 16]             256\n",
      "            ReLU-278          [-1, 128, 16, 16]               0\n",
      "          Conv2d-279          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-280          [-1, 128, 16, 16]             256\n",
      "            ReLU-281          [-1, 128, 16, 16]               0\n",
      "          Conv2d-282          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-283          [-1, 128, 16, 16]             256\n",
      "            ReLU-284          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-285          [-1, 128, 16, 16]               0\n",
      "          Conv2d-286          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-287          [-1, 128, 16, 16]             256\n",
      "            ReLU-288          [-1, 128, 16, 16]               0\n",
      "          Conv2d-289          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-290          [-1, 128, 16, 16]             256\n",
      "            ReLU-291          [-1, 128, 16, 16]               0\n",
      "          Conv2d-292          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-293          [-1, 128, 16, 16]             256\n",
      "            ReLU-294          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-295          [-1, 128, 16, 16]               0\n",
      "          Conv2d-296          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-297          [-1, 128, 16, 16]             256\n",
      "            ReLU-298          [-1, 128, 16, 16]               0\n",
      "          Conv2d-299          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-300          [-1, 128, 16, 16]             256\n",
      "            ReLU-301          [-1, 128, 16, 16]               0\n",
      "          Conv2d-302          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-303          [-1, 128, 16, 16]             256\n",
      "            ReLU-304          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-305          [-1, 128, 16, 16]               0\n",
      "          Conv2d-306          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-307          [-1, 128, 16, 16]             256\n",
      "            ReLU-308          [-1, 128, 16, 16]               0\n",
      "          Conv2d-309          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-310          [-1, 128, 16, 16]             256\n",
      "            ReLU-311          [-1, 128, 16, 16]               0\n",
      "          Conv2d-312          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-313          [-1, 128, 16, 16]             256\n",
      "            ReLU-314          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-315          [-1, 128, 16, 16]               0\n",
      "          Conv2d-316          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-317          [-1, 128, 16, 16]             256\n",
      "            ReLU-318          [-1, 128, 16, 16]               0\n",
      "          Conv2d-319          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-320          [-1, 128, 16, 16]             256\n",
      "            ReLU-321          [-1, 128, 16, 16]               0\n",
      "          Conv2d-322          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-323          [-1, 128, 16, 16]             256\n",
      "            ReLU-324          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-325          [-1, 128, 16, 16]               0\n",
      "          Conv2d-326          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-327          [-1, 128, 16, 16]             256\n",
      "            ReLU-328          [-1, 128, 16, 16]               0\n",
      "          Conv2d-329          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-330          [-1, 128, 16, 16]             256\n",
      "            ReLU-331          [-1, 128, 16, 16]               0\n",
      "          Conv2d-332          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-333          [-1, 128, 16, 16]             256\n",
      "            ReLU-334          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-335          [-1, 128, 16, 16]               0\n",
      "          Conv2d-336          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-337          [-1, 128, 16, 16]             256\n",
      "            ReLU-338          [-1, 128, 16, 16]               0\n",
      "          Conv2d-339          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-340          [-1, 128, 16, 16]             256\n",
      "            ReLU-341          [-1, 128, 16, 16]               0\n",
      "          Conv2d-342          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-343          [-1, 128, 16, 16]             256\n",
      "            ReLU-344          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-345          [-1, 128, 16, 16]               0\n",
      "          Conv2d-346          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-347          [-1, 128, 16, 16]             256\n",
      "            ReLU-348          [-1, 128, 16, 16]               0\n",
      "          Conv2d-349          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-350          [-1, 128, 16, 16]             256\n",
      "            ReLU-351          [-1, 128, 16, 16]               0\n",
      "          Conv2d-352          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-353          [-1, 128, 16, 16]             256\n",
      "            ReLU-354          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-355          [-1, 128, 16, 16]               0\n",
      "          Conv2d-356          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-357          [-1, 128, 16, 16]             256\n",
      "            ReLU-358          [-1, 128, 16, 16]               0\n",
      "          Conv2d-359          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-360          [-1, 128, 16, 16]             256\n",
      "            ReLU-361          [-1, 128, 16, 16]               0\n",
      "          Conv2d-362          [-1, 128, 16, 16]          16,384\n",
      "     BatchNorm2d-363          [-1, 128, 16, 16]             256\n",
      "            ReLU-364          [-1, 128, 16, 16]               0\n",
      "      Bottleneck-365          [-1, 128, 16, 16]               0\n",
      "          Conv2d-366          [-1, 256, 16, 16]          32,768\n",
      "     BatchNorm2d-367          [-1, 256, 16, 16]             512\n",
      "            ReLU-368          [-1, 256, 16, 16]               0\n",
      "          Conv2d-369            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-370            [-1, 256, 8, 8]             512\n",
      "            ReLU-371            [-1, 256, 8, 8]               0\n",
      "          Conv2d-372            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-373            [-1, 256, 8, 8]             512\n",
      "          Conv2d-374            [-1, 256, 8, 8]          32,768\n",
      "     BatchNorm2d-375            [-1, 256, 8, 8]             512\n",
      "            ReLU-376            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-377            [-1, 256, 8, 8]               0\n",
      "          Conv2d-378            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-379            [-1, 256, 8, 8]             512\n",
      "            ReLU-380            [-1, 256, 8, 8]               0\n",
      "          Conv2d-381            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-382            [-1, 256, 8, 8]             512\n",
      "            ReLU-383            [-1, 256, 8, 8]               0\n",
      "          Conv2d-384            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-385            [-1, 256, 8, 8]             512\n",
      "            ReLU-386            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-387            [-1, 256, 8, 8]               0\n",
      "          Conv2d-388            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-389            [-1, 256, 8, 8]             512\n",
      "            ReLU-390            [-1, 256, 8, 8]               0\n",
      "          Conv2d-391            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-392            [-1, 256, 8, 8]             512\n",
      "            ReLU-393            [-1, 256, 8, 8]               0\n",
      "          Conv2d-394            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-395            [-1, 256, 8, 8]             512\n",
      "            ReLU-396            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-397            [-1, 256, 8, 8]               0\n",
      "          Conv2d-398            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-399            [-1, 256, 8, 8]             512\n",
      "            ReLU-400            [-1, 256, 8, 8]               0\n",
      "          Conv2d-401            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-402            [-1, 256, 8, 8]             512\n",
      "            ReLU-403            [-1, 256, 8, 8]               0\n",
      "          Conv2d-404            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-405            [-1, 256, 8, 8]             512\n",
      "            ReLU-406            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-407            [-1, 256, 8, 8]               0\n",
      "          Conv2d-408            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-409            [-1, 256, 8, 8]             512\n",
      "            ReLU-410            [-1, 256, 8, 8]               0\n",
      "          Conv2d-411            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-412            [-1, 256, 8, 8]             512\n",
      "            ReLU-413            [-1, 256, 8, 8]               0\n",
      "          Conv2d-414            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-415            [-1, 256, 8, 8]             512\n",
      "            ReLU-416            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-417            [-1, 256, 8, 8]               0\n",
      "          Conv2d-418            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-419            [-1, 256, 8, 8]             512\n",
      "            ReLU-420            [-1, 256, 8, 8]               0\n",
      "          Conv2d-421            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-422            [-1, 256, 8, 8]             512\n",
      "            ReLU-423            [-1, 256, 8, 8]               0\n",
      "          Conv2d-424            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-425            [-1, 256, 8, 8]             512\n",
      "            ReLU-426            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-427            [-1, 256, 8, 8]               0\n",
      "          Conv2d-428            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-429            [-1, 256, 8, 8]             512\n",
      "            ReLU-430            [-1, 256, 8, 8]               0\n",
      "          Conv2d-431            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-432            [-1, 256, 8, 8]             512\n",
      "            ReLU-433            [-1, 256, 8, 8]               0\n",
      "          Conv2d-434            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-435            [-1, 256, 8, 8]             512\n",
      "            ReLU-436            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-437            [-1, 256, 8, 8]               0\n",
      "          Conv2d-438            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-439            [-1, 256, 8, 8]             512\n",
      "            ReLU-440            [-1, 256, 8, 8]               0\n",
      "          Conv2d-441            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-442            [-1, 256, 8, 8]             512\n",
      "            ReLU-443            [-1, 256, 8, 8]               0\n",
      "          Conv2d-444            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-445            [-1, 256, 8, 8]             512\n",
      "            ReLU-446            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-447            [-1, 256, 8, 8]               0\n",
      "          Conv2d-448            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-449            [-1, 256, 8, 8]             512\n",
      "            ReLU-450            [-1, 256, 8, 8]               0\n",
      "          Conv2d-451            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-452            [-1, 256, 8, 8]             512\n",
      "            ReLU-453            [-1, 256, 8, 8]               0\n",
      "          Conv2d-454            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-455            [-1, 256, 8, 8]             512\n",
      "            ReLU-456            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-457            [-1, 256, 8, 8]               0\n",
      "          Conv2d-458            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-459            [-1, 256, 8, 8]             512\n",
      "            ReLU-460            [-1, 256, 8, 8]               0\n",
      "          Conv2d-461            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-462            [-1, 256, 8, 8]             512\n",
      "            ReLU-463            [-1, 256, 8, 8]               0\n",
      "          Conv2d-464            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-465            [-1, 256, 8, 8]             512\n",
      "            ReLU-466            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-467            [-1, 256, 8, 8]               0\n",
      "          Conv2d-468            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-469            [-1, 256, 8, 8]             512\n",
      "            ReLU-470            [-1, 256, 8, 8]               0\n",
      "          Conv2d-471            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-472            [-1, 256, 8, 8]             512\n",
      "            ReLU-473            [-1, 256, 8, 8]               0\n",
      "          Conv2d-474            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-475            [-1, 256, 8, 8]             512\n",
      "            ReLU-476            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-477            [-1, 256, 8, 8]               0\n",
      "          Conv2d-478            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-479            [-1, 256, 8, 8]             512\n",
      "            ReLU-480            [-1, 256, 8, 8]               0\n",
      "          Conv2d-481            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-482            [-1, 256, 8, 8]             512\n",
      "            ReLU-483            [-1, 256, 8, 8]               0\n",
      "          Conv2d-484            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-485            [-1, 256, 8, 8]             512\n",
      "            ReLU-486            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-487            [-1, 256, 8, 8]               0\n",
      "          Conv2d-488            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-489            [-1, 256, 8, 8]             512\n",
      "            ReLU-490            [-1, 256, 8, 8]               0\n",
      "          Conv2d-491            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-492            [-1, 256, 8, 8]             512\n",
      "            ReLU-493            [-1, 256, 8, 8]               0\n",
      "          Conv2d-494            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-495            [-1, 256, 8, 8]             512\n",
      "            ReLU-496            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-497            [-1, 256, 8, 8]               0\n",
      "          Conv2d-498            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-499            [-1, 256, 8, 8]             512\n",
      "            ReLU-500            [-1, 256, 8, 8]               0\n",
      "          Conv2d-501            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-502            [-1, 256, 8, 8]             512\n",
      "            ReLU-503            [-1, 256, 8, 8]               0\n",
      "          Conv2d-504            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-505            [-1, 256, 8, 8]             512\n",
      "            ReLU-506            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-507            [-1, 256, 8, 8]               0\n",
      "          Conv2d-508            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-509            [-1, 256, 8, 8]             512\n",
      "            ReLU-510            [-1, 256, 8, 8]               0\n",
      "          Conv2d-511            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-512            [-1, 256, 8, 8]             512\n",
      "            ReLU-513            [-1, 256, 8, 8]               0\n",
      "          Conv2d-514            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-515            [-1, 256, 8, 8]             512\n",
      "            ReLU-516            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-517            [-1, 256, 8, 8]               0\n",
      "          Conv2d-518            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-519            [-1, 256, 8, 8]             512\n",
      "            ReLU-520            [-1, 256, 8, 8]               0\n",
      "          Conv2d-521            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-522            [-1, 256, 8, 8]             512\n",
      "            ReLU-523            [-1, 256, 8, 8]               0\n",
      "          Conv2d-524            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-525            [-1, 256, 8, 8]             512\n",
      "            ReLU-526            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-527            [-1, 256, 8, 8]               0\n",
      "          Conv2d-528            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-529            [-1, 256, 8, 8]             512\n",
      "            ReLU-530            [-1, 256, 8, 8]               0\n",
      "          Conv2d-531            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-532            [-1, 256, 8, 8]             512\n",
      "            ReLU-533            [-1, 256, 8, 8]               0\n",
      "          Conv2d-534            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-535            [-1, 256, 8, 8]             512\n",
      "            ReLU-536            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-537            [-1, 256, 8, 8]               0\n",
      "          Conv2d-538            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-539            [-1, 256, 8, 8]             512\n",
      "            ReLU-540            [-1, 256, 8, 8]               0\n",
      "          Conv2d-541            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-542            [-1, 256, 8, 8]             512\n",
      "            ReLU-543            [-1, 256, 8, 8]               0\n",
      "          Conv2d-544            [-1, 256, 8, 8]          65,536\n",
      "     BatchNorm2d-545            [-1, 256, 8, 8]             512\n",
      "            ReLU-546            [-1, 256, 8, 8]               0\n",
      "      Bottleneck-547            [-1, 256, 8, 8]               0\n",
      "     BatchNorm2d-548            [-1, 256, 8, 8]             512\n",
      "            ReLU-549            [-1, 256, 8, 8]               0\n",
      "AdaptiveAvgPool2d-550            [-1, 256, 1, 1]               0\n",
      "          Linear-551                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 17,081,914\n",
      "Trainable params: 17,081,914\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 163.00\n",
      "Params size (MB): 65.16\n",
      "Estimated Total Size (MB): 228.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from resnet_2 import resnet164\n",
    "\n",
    "import torchsummary\n",
    "\n",
    "model = resnet164(case=0)\n",
    "model.cuda()\n",
    "torchsummary.summary(model,(3,32,32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.eye(4000)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[129], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# using np.multinomial() method\u001B[39;00m\n\u001B[0;32m      4\u001B[0m gfg \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mmultivariate_normal(mean, matrix, \u001B[38;5;241m2000\u001B[39m) \u001B[38;5;66;03m#5生成5个样本点\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m _,v,_\u001B[38;5;241m=\u001B[39m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msvd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatrix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m v\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36msvd\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\pytorch39\\lib\\site-packages\\numpy\\linalg\\linalg.py:1657\u001B[0m, in \u001B[0;36msvd\u001B[1;34m(a, full_matrices, compute_uv, hermitian)\u001B[0m\n\u001B[0;32m   1654\u001B[0m         gufunc \u001B[38;5;241m=\u001B[39m _umath_linalg\u001B[38;5;241m.\u001B[39msvd_n_s\n\u001B[0;32m   1656\u001B[0m signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD->DdD\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m isComplexType(t) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124md->ddd\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m-> 1657\u001B[0m u, s, vh \u001B[38;5;241m=\u001B[39m \u001B[43mgufunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1658\u001B[0m u \u001B[38;5;241m=\u001B[39m u\u001B[38;5;241m.\u001B[39mastype(result_t, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1659\u001B[0m s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mastype(_realType(result_t), copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "mean = np.zeros([2000]) #均值向量\n",
    "matrix = np.eye(2000)\n",
    "# using np.multinomial() method\n",
    "gfg = np.random.multivariate_normal(mean, matrix, 4000) #5生成5个样本点\n",
    "_,v,_=np.linalg.svd(matrix)\n",
    "v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = np.cov(gfg.T)\n",
    "_,v,_=np.linalg.svd(cov)\n",
    "v=v[np.argwhere(v>1e-9)]\n",
    "len(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 960x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=150)\n",
    "n,bins,_=plt.hist(v,bins=50)\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"λ\")\n",
    "plt.ylabel(\"hist\")\n",
    "plt.savefig('特征值.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}